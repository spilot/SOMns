# Config file for ReBench
# Config format is YAML (see http://yaml.org/ for detailed spec)

# this run definition will be choosen if no parameters are given to rebench.py
standard_run: all
standard_data_file: 'codespeed.data'
standard_experiment: SOMns

runs:
    number_of_data_points: 500

statistics:
    confidence_level: 0.95

reporting:
    # results can also be reported to a codespeed instance
    # see: https://github.com/tobami/codespeed
    csv_file: latest-runs.csv
    csv_locale: de_DE.UTF-8
    codespeed:
        url: http://codespeed.iedri.ch/result/add/json/

# definition of benchmark suites
benchmark_suites:
    macro-steady:
        gauge_adapter: RebenchLog
        command: &MACRO_CMD " core-lib/Benchmarks/Harness.ns %(benchmark)s "
        max_runtime: 6000
        benchmarks:
            - CD:
                extra_args: "350 0 10" # TODO: verification results only for 2 and 10
                codespeed_name: "peak.CD"
                warmup: &WARMUP 100
            - Havlak:
                extra_args: "350 0 5" # TODO: verification results only for 1 and 5
                codespeed_name: "peak.Havlak"
                warmup: *WARMUP
            - Richards:
                extra_args: "350 0 1"
                codespeed_name: "peak.Richards"
                warmup: *WARMUP
            - RichardsNS:
                extra_args: "350 0 10"
                codespeed_name: "peak.RichardsNS"
                warmup: *WARMUP
            - DeltaBlue:
                extra_args: "350 0 1200"
                codespeed_name: "peak.DeltaBlue"
                warmup: *WARMUP
#            - DeltaBlueNS: # TODO: what is that?
#                extra_args: "250 0 1000"
#                codespeed_name: "peak.DeltaBlue"
#                warmup: 150
            - Mandelbrot:
                extra_args: "350 0 150"
                codespeed_name: "peak.Mandelbrot"
                warmup: *WARMUP
            - NBody:
                extra_args: "350 0 10000"
                codespeed_name: "peak.NBody"
                warmup: *WARMUP
            - Json:
                extra_args: "350 0 5"
                codespeed_name: "peak.Json"
                warmup: *WARMUP
            - GraphSearch:
                extra_args: "350 0 3"
                codespeed_name: "peak.GraphSearch"
                warmup: *WARMUP
            - PageRank:
                extra_args: "350 0 250"
                codespeed_name: "peak.PageRank"
                warmup: *WARMUP
            - LeeTM:
                extra_args: "350 0 1"
                codespeed_name: "peak.Lee"
                warmup: *WARMUP
            - Vacation:
                extra_args: "350 0 10"
                codespeed_name: "peak.Vacation"
                warmup: *WARMUP
            - Splay:
                extra_args: "350 0 1"
                codespeed_name: "peak.Splay"
                warmup: *WARMUP
            - Fannkuch:
                extra_args: "350 0 7"
                codespeed_name: "peak.Fannkuch"
                warmup: *WARMUP
            - List:
                extra_args: "350 0 50"
                codespeed_name: "peak.List"
                warmup: *WARMUP
            - Bounce:
                extra_args: "350 0 50"
                codespeed_name: "peak.Bounce"
                warmup: *WARMUP
            - Permute:
                extra_args: "350 0 50"
                codespeed_name: "peak.Permute"
                warmup: *WARMUP
            - Queens:
                extra_args: "350 0 40"
                codespeed_name: "peak.Queens"
                warmup: *WARMUP
            - Storage:
                extra_args: "350 0 50"
                codespeed_name: "peak.Storage"
                warmup: *WARMUP
            - Sieve:
                extra_args: "350 0 200"
                codespeed_name: "peak.Sieve"
                warmup: *WARMUP
            - Towers:
                extra_args: "350 0 30"
                codespeed_name: "peak.Towers"
                warmup: *WARMUP

# VMs have a name and are specified by a path and the binary to be executed
virtual_machines:
    SOMns-interp-baseline:
        path: .
        binary: som
        args: "-G -Dsom.superinstructions=false"
    SOMns-interp-super:
        path: .
        binary: som
        args: "-G "

# define the benchmarks to be executed for a re-executable benchmark run
experiments:
    SOMns-interp:
        description: All benchmarks on SOMns without Graal
        actions: benchmark
        benchmark:
            - macro-steady
        executions:
            - SOMns-interp-baseline
            - SOMns-interp-super


